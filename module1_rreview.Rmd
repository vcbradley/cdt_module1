---
title: "CDT Module 1 - R Review"
author: "Valerie Bradley"
date: "9/19/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(data.table)
```

## 1. Vectors

a. Generate 100 standard normal random variables, and keep only the ones which are greater than 1. Donâ€™t use a loop!


Problem with this is that the number of R.V.s generated 
```{r cars}
all_norm = rnorm(1000)
hist(all_norm)
hist(all_norm[all_norm > 1], breaks = 10)

truncNorm <- function(n, min){
  out <- c()
  
  while(length(out) < n){
    tmp <- rnorm(n)
    tmp <- tmp[tmp > min]
    out <- c(out, tmp)   #will copy out each time -- uses more memory.  alternately, you could initialize
  }
  
  out[seq_len(n)]  #sequence of length n
}

hist(truncNorm(100,1), breaks = 15)
```

b. Write a function which takes two arguments n and min, and returns n independent random variables from a standard normal distribution truncated below by min. Let min default to 0.
```{r 1b}
rnorm_trunc = function(n, min = 0){
  X = rnorm(n)
  X = unlist(lapply(X, max, min))
  return(X)
}

hist(rnorm_trunc(2000, 1))
```


c. Generate 10k truncated normals with min set at -1 and plot as histogram
```{r}
samp = rnorm_trunc(10000, -1)
hist(samp, breaks  = 10)
```


d. what happens if min is large?
```{r}
hist(rnorm_trunc(10000,4))
```

Point here is that this method of rejection sampling is inefficient 


### 2. Data

```{r}
library(MASS)
data(hills)
summary(hills)
```

a. what sort of object is hills?
```{r}
class(hills)
```

b. how many columns?
```{r}
ncol(hills)
```

c. Change "Two Breweries" to "Three Breweries"
```{r}
hills[which(rownames(hills) == 'Two Breweries'),]
row.names(hills)[which(rownames(hills) == 'Two Breweries')] <-'Three Breweries'
hills[which(rownames(hills) == 'Two Breweries'),]
hills[which(rownames(hills) == 'Three Breweries'),]
```

d. Find the mean time for races iwth a climb greater than 1000ft
```{r}
with(hills[hills$climb > 1000,], mean(dist))
```

e. What sort of object is Orthodont?  How is it different from `hills`?
```{r}
library(nlme)
data(Orthodont)
head(Orthodont)
#class(Orthodont)
```
It's grouped data.


```{r}
head(methods(print))
#nlme:::print.groupedData(Orthodont)
```




### 3. Recursion

Important point here: R is bad a recursion.

The $n$th Fibonacci number is defined by the recusion $F_n=F_{n-1} + F_{n-2}$ with $F_0=F_1=1$

a. Write a recursive function with argument `n` which returns the $n$th Fibonacci number

```{r}
fibonacci = function(n){
  if(n < 0) {
    #print(n)
    F_n = 0
  }else if(n <= 1){  #note this is different than the example given in Recall (they used <=2) - has to do with indexing from 0 v 1
    #print(n)
    F_n = 1
  }else{
    #print(n)
    F_n = Recall(n-1) + Recall(n-2)
  }
  F_n
}

fibonacci(2)   #evaluated 3 times

fibonacci(0)   #evaluated 1 time
fibonacci(1)   #evaluated 1 time
fibonacci(3)   #evaluated 5 times
```


```{r}
fib_2 = function(n){
  fib_seq = c(1,2)
  
  if(n < 0) return(0)
  if(n <= 1) return(1)
  if(n == 2) return(2)
  
  for(i in 3:n){
       fib_seq[i] <- fib_seq[i-1] + fib_seq[i-2]
  }
  
  return(fib_seq[n])
}
fib_2(1)
fib_2(2)
fib_2(3)
fib_2(4)
fib_2(5)
fib_2(100)
```


### 4. MCMC
a. 
X ~ Gamma(alpha, beta)
alpha, beta ~ Exp(1)

```{r}
#vector of data x
alpha_true = 1
beta_true = 2

x = rgamma(100, shape = alpha_true, rate = beta_true)


# evaluate the posterior distribution of alpha given a vector of data
posterior = function(x, alpha, beta){
  
  # get prior density
  prior_alpha = dexp(alpha, rate = 1, log = T)  # alpha ~ exp(1)
  prior_beta = dexp(beta, rate = 1, log = T)  # beta ~ exp(1)
  
  likelihood = dgamma(x, alpha, beta, log = T)
  
  posterior = prior_alpha + prior_beta + sum(likelihood)
  
  return(posterior)
}

posterior(x, alpha = 1.1, beta = 2.3)
posterior(x, alpha = 2, beta = 2)

```


b. single metropolis hasting step
$\alpha' = \alpha + \sigma Z_1$
$\beta' = \beta + \sigma Z_1$
where $Z_1 ,Z_2$ are iid Standard Normal $q'(\alpha' | \alpha) ~ N(\alpha, \sigma^2)$
```{r}
step_MH = function(x, alpha, beta, sigma){
  
  alpha_prime = rnorm(n = 1, alpha, sigma)
  beta_prime = rnorm(n = 1, beta, sigma)
  
  a = exp(posterior(x, alpha = alpha_prime, beta = beta_prime) - posterior(x, alpha = alpha, beta = beta))
  u = runif(1)
  if(u < a){
    data.frame(alpha = alpha_prime, beta = beta_prime)
  }else{
    data.frame(alpha = alpha, beta = beta)
  }
}

step_MH(x, alpha = 1.1, beta = 2.3, sigma = 0.01)

```

```{r}
run_MH = function(N, x, alpha, beta, sigma){
  chains = data.frame(t = 0, alpha = alpha, beta = beta)
  for(t in 1:N){
    step = step_MH(x, alpha = chains[t,]$alpha, beta = chains[t,]$beta, sigma)
    chains[t+1, ] = c(t, step$alpha, step$beta)
  }
  return(chains)
}
```

Now, run the full M-H algorithm and plot the resulting chains
```{r}
chains = run_MH(5000, x = x, alpha = 3, beta = 1, sigma = 2/sqrt(length(x)))

ggplot(chains, aes(x = t, y = alpha)) + geom_line(aes(color = 'alpha')) + geom_line(aes(x = t, y = beta, color = 'beta')) +
  ggtitle(paste0("Metropolis-Hastings Sampling for Gamma(alpha, beta)\n", "alpha = ", alpha_true, ", beta = ", beta_true)) + 
  geom_hline(yintercept = alpha_true) +
  geom_hline(yintercept = beta_true) +
  ylab("Parameter estimate")
```

Plot the posterior distributions of the parameters
```{r}
ggplot(chains) + stat_density(aes(x = alpha, fill = 'alpha', alpha = 0.1)) +
  stat_density(aes(x = beta, fill = 'beta', alpha = 0.1)) +
  geom_vline(xintercept = mean(chains$alpha), linetype = 'dotted') + 
  geom_vline(xintercept = alpha_true) +
  geom_vline(xintercept = mean(chains$beta), linetype = 'dotted') + 
  geom_vline(xintercept = beta_true) +
  ggtitle(paste0("M-H Sampling\nPosterior parameter distributions for Gamma(alpha, beta)\nT = ", nrow(chains) - 1))
```

Let's see how this converges over time
```{r}
Ts = seq(250, 5000, by = 250)

all_chains = lapply(Ts, FUN = run_MH, x = x, alpha = 5, beta = 0.3, sigma = 0.01)

all_chains = rbindlist(all_chains)
all_chains = as.data.table(all_chains)
all_chains[, N := rep(Ts, times = Ts + 1)]


all_chains[, .(mean(alpha), .N), N]
all_chains[, .(mean(beta), .N), N]

ggplot(all_chains[, .(alpha_hat = mean(alpha)), N]) + geom_point(aes(x = N, y = alpha_hat)) + geom_hline(yintercept = alpha_true)
ggplot(all_chains[, .(beta_hat = mean(beta)), N]) + geom_point(aes(x = N, y = beta_hat)) + geom_hline(yintercept = beta_true)

```


d. Air pollution data.  
```{r}
x <- scan("/Users/valeriebradley/Documents/Oxford/Module 1/R review/airpol.txt")
hist(x, breaks = 100, freq = FALSE, main = "Distribution of daily PM2.5 readings in Seattle, 2015", xlab = "PM2.5")
```

Model data as iid Gamma using Exp(1) priors from above.  Run for 5,000 iterations with starting points $\alpha = 1$, $\beta = 1$.
```{r}
sigmas = c(0.01, 0.02, 0.05)
chains_airpol = lapply(sigmas, FUN = run_MH, N = 5000, x = x, alpha = 1, beta = 1)

chains_airpol = rbindlist(chains_airpol)
chains_airpol = as.data.table(chains_airpol)
chains_airpol[, sigma := sort(rep(sigmas, times = 5000 + 1))]

ggplot(chains_airpol) + geom_line(aes(x = t, y = alpha, group = sigma, color = as.factor(sigma))) +
  ggtitle("Chain convergence of parameter alpha")
```


```{r}
ggplot(chains_airpol) + geom_line(aes(x = t, y = beta, group = sigma, color = as.factor(sigma))) +
  ggtitle("Chain convergence of parameter beta")
```

```{r}
ggplot(chains_airpol, aes(x = alpha, y = beta)) + geom_point() + 
  ggtitle("M-H Exploration of the (alpha, beta) parameter space") +
  geom_density2d()
```



e. Find posterior means of $\alpha$ and $\beta$.  Plot density of corresponding Gamma distribution over the histogram of the data.
```{r}
chains_airpol[, lapply(.SD, mean), .SDcols = c('alpha', 'beta')]

ggplot(data.frame(x = x)) + stat_density(aes(x = x, fill = 'data', alpha = 0.2)) + 
  stat_function(fun = dgamma, args = list(shape = chains_airpol[, mean(alpha)], rate = chains_airpol[, mean(beta)]), aes(color = 'posterior')) +
  scale_colour_manual("",values = 'black') +
  ggtitle('Comparison of data and estimated posterior')
```



## 5. Methods


Note: solutions below use S4 methods
```{r}
setClass('biv', representation(x = "numeric", y = "numeric"))
new_df = new('biv', x = rnorm(n = 20), y = rpois(n = 20, lambda = 5))
new_df
```

Create a print method (that invisibly returns the object)
```{r}
setMethod('print', 'biv', function(x){
  cat(paste0("Bivariate data, ", length(x@x), " entries\n"))
  cat(paste('x :' , paste(x@x[1:min(length(x@x),6)], collapse = " "), "...\n"))
  cat(paste('y :' , paste(x@y[1:min(length(x@y),6)], collapse = " "), "..."))
  invisible(x)
})
print(new_df)
```

Construct a plot method for class `biv`
```{r}
setMethod('plot', 'biv', function(x){
  layout(matrix(c(1,2), 1, 2, byrow = TRUE))
  plot(x = x@x, y = x@y, xlab = 'x', ylab = 'y')
  boxplot(x@x, x@y)
})
plot(new_df)
```

Notes:
- S4 methods won't have `names()`, instead use `slotNames()`
- S4 methods are slightly slower, but maybe more stable?
- use the `@` operator to access slots -- don't make your user use the `@` (create methods instead)



## 6. Functions
Create a function to return a model matrix from 2 variables
```{r}
getSimpleMatrix = function(x,z){
  if(length(x) != length(z)) stop(print("x and z must have the same number of entries"))
  
  n_obs = length(x)
  intercept = rep(1, n_obs)
  predictors = c(x,z)
  interactions = x * z
  
  return(matrix(c(intercept, predictors, interactions), ncol = 4, byrow = F))
}

getSimpleMatrix(x = c(1,2,3,4,5), z = c(6,7,8,9,0))

```

Check with model.matrix
```{r}
model.matrix(as.formula(~1+x+z+x:z), data = data.frame(x = c(1,2,3,4,5), z = c(6,7,8,9,0)))
```

Create a function to return a model matrix from n variables
```{r}
getMatrix = function(...){
  inputs = list(...)
  
  #check input length
  if(length(unique(unlist(lapply(inputs, length)))) > 1) 
    stop("All inputs must have the same number of observations")
  
  n_obs = length(x)
  n_vars = length(inputs)
  
 predictors = matrix(unlist(inputs), ncol = n_vars, byrow = F)
 interactions = apply(combn(x = n_vars, 2), 2, function(x){
    inputs[[x[1]]] * inputs[[x[2]]]
  })
 
 return(cbind(rep(1, n_obs), predictors, interactions))
}

getMatrix(x = c(1,2,3,4,5), z = c(6,7,8,9,0), y = c(2,4,6,8,10))
```

###7. Mixtures
$X^{(i)}  = (X_{i1},...,X_{ik})$ where each $X_{ij}$ is binary
A discrete mixture model assumes that each component of the vector $X^(i)$ is independent, conditional upon an unknown class label $U_i \in \{1,...,l\}$
a. write down the likelihood for one observation $X^{(1)}$, and then for $n$ observations.  What are the parameters to be estimated?
$$L()$$
b.
```{r}

```


